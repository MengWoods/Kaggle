{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'sample_submission.csv', 'test.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold # crossvalidation wrt class ratio\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, scale\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, GlobalMaxPool2D, BatchNormalization, Input, Conv2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import metrics\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy\n",
    "import gc # trash return\n",
    "import scipy.special\n",
    "from tqdm import *  # processing bar\n",
    "from scipy.stats import norm, rankdata\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 1024\n",
    "NUM_FEATURES = 600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "## 1-Load dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "03b40cceaa3bed3bc690111b5ac0126cea41e5a3"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "f55bb756adb00a86159e3819c2c6229010c08d27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape (200000, 202) \n",
      "test.shape (200000, 201)\n"
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "print('train.shape', train.shape, '\\ntest.shape', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "d58cc8c71b213c46f59343ea0259602f44572bfd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 202 entries, ID_code to var_199\n",
      "dtypes: float64(200), int64(1), object(1)\n",
      "memory usage: 308.2+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 200000 entries, 0 to 199999\n",
      "Columns: 201 entries, ID_code to var_199\n",
      "dtypes: float64(200), object(1)\n",
      "memory usage: 306.7+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print ( train.info())\n",
    "print ( test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "d18652144725ce887a0ffc6ae77a35ab3267815a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       202\n",
       "unique        1\n",
       "top       False\n",
       "freq        202\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.isnull().any().describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b5f43fa5d27510c732f5710afd08b90ff9253497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       201\n",
       "unique        1\n",
       "top       False\n",
       "freq        201\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "ccc0b053bd01238ac9345cc21d73be54466c8ee6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>var_11</th>\n",
       "      <th>var_12</th>\n",
       "      <th>var_13</th>\n",
       "      <th>var_14</th>\n",
       "      <th>var_15</th>\n",
       "      <th>var_16</th>\n",
       "      <th>var_17</th>\n",
       "      <th>var_18</th>\n",
       "      <th>var_19</th>\n",
       "      <th>var_20</th>\n",
       "      <th>var_21</th>\n",
       "      <th>var_22</th>\n",
       "      <th>var_23</th>\n",
       "      <th>var_24</th>\n",
       "      <th>var_25</th>\n",
       "      <th>var_26</th>\n",
       "      <th>var_27</th>\n",
       "      <th>var_28</th>\n",
       "      <th>var_29</th>\n",
       "      <th>var_30</th>\n",
       "      <th>var_31</th>\n",
       "      <th>var_32</th>\n",
       "      <th>var_33</th>\n",
       "      <th>var_34</th>\n",
       "      <th>var_35</th>\n",
       "      <th>var_36</th>\n",
       "      <th>var_37</th>\n",
       "      <th>...</th>\n",
       "      <th>var_160</th>\n",
       "      <th>var_161</th>\n",
       "      <th>var_162</th>\n",
       "      <th>var_163</th>\n",
       "      <th>var_164</th>\n",
       "      <th>var_165</th>\n",
       "      <th>var_166</th>\n",
       "      <th>var_167</th>\n",
       "      <th>var_168</th>\n",
       "      <th>var_169</th>\n",
       "      <th>var_170</th>\n",
       "      <th>var_171</th>\n",
       "      <th>var_172</th>\n",
       "      <th>var_173</th>\n",
       "      <th>var_174</th>\n",
       "      <th>var_175</th>\n",
       "      <th>var_176</th>\n",
       "      <th>var_177</th>\n",
       "      <th>var_178</th>\n",
       "      <th>var_179</th>\n",
       "      <th>var_180</th>\n",
       "      <th>var_181</th>\n",
       "      <th>var_182</th>\n",
       "      <th>var_183</th>\n",
       "      <th>var_184</th>\n",
       "      <th>var_185</th>\n",
       "      <th>var_186</th>\n",
       "      <th>var_187</th>\n",
       "      <th>var_188</th>\n",
       "      <th>var_189</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_10</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0702</td>\n",
       "      <td>-0.5447</td>\n",
       "      <td>9.5900</td>\n",
       "      <td>4.2987</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-18.8687</td>\n",
       "      <td>6.0382</td>\n",
       "      <td>14.3797</td>\n",
       "      <td>-0.4711</td>\n",
       "      <td>7.3198</td>\n",
       "      <td>4.6603</td>\n",
       "      <td>-14.0548</td>\n",
       "      <td>13.9059</td>\n",
       "      <td>9.0796</td>\n",
       "      <td>11.8218</td>\n",
       "      <td>14.3125</td>\n",
       "      <td>11.0386</td>\n",
       "      <td>4.7462</td>\n",
       "      <td>17.4442</td>\n",
       "      <td>10.7502</td>\n",
       "      <td>11.6244</td>\n",
       "      <td>8.2466</td>\n",
       "      <td>-0.5277</td>\n",
       "      <td>2.8072</td>\n",
       "      <td>7.4451</td>\n",
       "      <td>13.7830</td>\n",
       "      <td>-0.2494</td>\n",
       "      <td>-0.1141</td>\n",
       "      <td>5.9583</td>\n",
       "      <td>2.8036</td>\n",
       "      <td>-0.2332</td>\n",
       "      <td>12.5400</td>\n",
       "      <td>2.1982</td>\n",
       "      <td>11.5565</td>\n",
       "      <td>11.0755</td>\n",
       "      <td>5.4034</td>\n",
       "      <td>8.5073</td>\n",
       "      <td>6.9892</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0656</td>\n",
       "      <td>5.3470</td>\n",
       "      <td>3.7800</td>\n",
       "      <td>8.0339</td>\n",
       "      <td>-9.9638</td>\n",
       "      <td>20.9325</td>\n",
       "      <td>3.7723</td>\n",
       "      <td>-3.8352</td>\n",
       "      <td>7.4966</td>\n",
       "      <td>5.7487</td>\n",
       "      <td>0.4344</td>\n",
       "      <td>6.5370</td>\n",
       "      <td>7.2105</td>\n",
       "      <td>-3.5162</td>\n",
       "      <td>27.1062</td>\n",
       "      <td>13.2255</td>\n",
       "      <td>1.1165</td>\n",
       "      <td>10.2981</td>\n",
       "      <td>-4.0424</td>\n",
       "      <td>3.9860</td>\n",
       "      <td>-11.6023</td>\n",
       "      <td>11.1783</td>\n",
       "      <td>14.8384</td>\n",
       "      <td>10.9622</td>\n",
       "      <td>22.4111</td>\n",
       "      <td>-3.3333</td>\n",
       "      <td>16.2396</td>\n",
       "      <td>-7.0216</td>\n",
       "      <td>17.3210</td>\n",
       "      <td>0.6963</td>\n",
       "      <td>7.2780</td>\n",
       "      <td>8.0819</td>\n",
       "      <td>0.2139</td>\n",
       "      <td>-0.0761</td>\n",
       "      <td>14.9585</td>\n",
       "      <td>-1.2160</td>\n",
       "      <td>6.6576</td>\n",
       "      <td>9.2553</td>\n",
       "      <td>14.2914</td>\n",
       "      <td>-7.6652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_11</td>\n",
       "      <td>0</td>\n",
       "      <td>12.7188</td>\n",
       "      <td>-7.9750</td>\n",
       "      <td>10.3757</td>\n",
       "      <td>9.0101</td>\n",
       "      <td>12.8570</td>\n",
       "      <td>-12.0852</td>\n",
       "      <td>5.6464</td>\n",
       "      <td>11.8370</td>\n",
       "      <td>1.2953</td>\n",
       "      <td>6.8093</td>\n",
       "      <td>-6.1501</td>\n",
       "      <td>-5.4925</td>\n",
       "      <td>13.6713</td>\n",
       "      <td>9.5331</td>\n",
       "      <td>4.8230</td>\n",
       "      <td>14.7383</td>\n",
       "      <td>6.8414</td>\n",
       "      <td>-9.0195</td>\n",
       "      <td>17.5407</td>\n",
       "      <td>17.1926</td>\n",
       "      <td>6.0760</td>\n",
       "      <td>2.3546</td>\n",
       "      <td>7.6435</td>\n",
       "      <td>3.9800</td>\n",
       "      <td>6.9099</td>\n",
       "      <td>13.3506</td>\n",
       "      <td>-8.0681</td>\n",
       "      <td>-3.9470</td>\n",
       "      <td>3.9651</td>\n",
       "      <td>7.7577</td>\n",
       "      <td>-6.7792</td>\n",
       "      <td>11.9451</td>\n",
       "      <td>-3.2295</td>\n",
       "      <td>10.7779</td>\n",
       "      <td>10.8248</td>\n",
       "      <td>-5.8872</td>\n",
       "      <td>-0.2503</td>\n",
       "      <td>6.7759</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9874</td>\n",
       "      <td>5.4122</td>\n",
       "      <td>3.0756</td>\n",
       "      <td>12.5066</td>\n",
       "      <td>1.5809</td>\n",
       "      <td>18.2420</td>\n",
       "      <td>3.6452</td>\n",
       "      <td>-3.6111</td>\n",
       "      <td>3.4261</td>\n",
       "      <td>5.7226</td>\n",
       "      <td>2.7449</td>\n",
       "      <td>-0.3629</td>\n",
       "      <td>23.9603</td>\n",
       "      <td>-4.3861</td>\n",
       "      <td>22.4637</td>\n",
       "      <td>10.8169</td>\n",
       "      <td>-7.0860</td>\n",
       "      <td>13.8536</td>\n",
       "      <td>-5.3953</td>\n",
       "      <td>6.3811</td>\n",
       "      <td>-0.2817</td>\n",
       "      <td>11.1792</td>\n",
       "      <td>3.9031</td>\n",
       "      <td>14.1906</td>\n",
       "      <td>14.7535</td>\n",
       "      <td>-10.4444</td>\n",
       "      <td>6.8885</td>\n",
       "      <td>-0.1458</td>\n",
       "      <td>21.8375</td>\n",
       "      <td>-0.1857</td>\n",
       "      <td>-0.8901</td>\n",
       "      <td>2.6559</td>\n",
       "      <td>-0.0503</td>\n",
       "      <td>5.5946</td>\n",
       "      <td>13.6152</td>\n",
       "      <td>2.4058</td>\n",
       "      <td>-1.7183</td>\n",
       "      <td>9.6745</td>\n",
       "      <td>16.7498</td>\n",
       "      <td>-3.9728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_12</td>\n",
       "      <td>0</td>\n",
       "      <td>8.7671</td>\n",
       "      <td>-4.6154</td>\n",
       "      <td>9.7242</td>\n",
       "      <td>7.4242</td>\n",
       "      <td>9.0254</td>\n",
       "      <td>1.4247</td>\n",
       "      <td>6.2815</td>\n",
       "      <td>12.3143</td>\n",
       "      <td>5.6964</td>\n",
       "      <td>6.0197</td>\n",
       "      <td>5.2524</td>\n",
       "      <td>-4.5162</td>\n",
       "      <td>14.1985</td>\n",
       "      <td>9.6978</td>\n",
       "      <td>8.6948</td>\n",
       "      <td>13.9111</td>\n",
       "      <td>9.3556</td>\n",
       "      <td>-13.7217</td>\n",
       "      <td>2.1749</td>\n",
       "      <td>11.4645</td>\n",
       "      <td>9.4219</td>\n",
       "      <td>34.8112</td>\n",
       "      <td>6.4927</td>\n",
       "      <td>4.0522</td>\n",
       "      <td>12.5483</td>\n",
       "      <td>13.5935</td>\n",
       "      <td>-9.9662</td>\n",
       "      <td>-2.0558</td>\n",
       "      <td>5.6994</td>\n",
       "      <td>11.3379</td>\n",
       "      <td>4.0196</td>\n",
       "      <td>8.3829</td>\n",
       "      <td>3.6990</td>\n",
       "      <td>23.8890</td>\n",
       "      <td>12.0540</td>\n",
       "      <td>0.9791</td>\n",
       "      <td>0.7492</td>\n",
       "      <td>5.3935</td>\n",
       "      <td>...</td>\n",
       "      <td>30.5234</td>\n",
       "      <td>5.7813</td>\n",
       "      <td>6.4252</td>\n",
       "      <td>20.5597</td>\n",
       "      <td>-3.8090</td>\n",
       "      <td>20.3979</td>\n",
       "      <td>3.3998</td>\n",
       "      <td>-12.2034</td>\n",
       "      <td>8.4840</td>\n",
       "      <td>5.4729</td>\n",
       "      <td>3.2530</td>\n",
       "      <td>4.7714</td>\n",
       "      <td>18.5149</td>\n",
       "      <td>-1.3783</td>\n",
       "      <td>27.0374</td>\n",
       "      <td>10.3448</td>\n",
       "      <td>-7.8723</td>\n",
       "      <td>14.0301</td>\n",
       "      <td>-14.0628</td>\n",
       "      <td>-1.2209</td>\n",
       "      <td>8.6217</td>\n",
       "      <td>10.2952</td>\n",
       "      <td>-16.5506</td>\n",
       "      <td>6.9453</td>\n",
       "      <td>-0.5242</td>\n",
       "      <td>-1.3825</td>\n",
       "      <td>10.0550</td>\n",
       "      <td>-13.6977</td>\n",
       "      <td>20.0393</td>\n",
       "      <td>1.2974</td>\n",
       "      <td>0.3782</td>\n",
       "      <td>7.4382</td>\n",
       "      <td>0.0854</td>\n",
       "      <td>1.3444</td>\n",
       "      <td>17.2439</td>\n",
       "      <td>-0.0798</td>\n",
       "      <td>5.7389</td>\n",
       "      <td>8.4897</td>\n",
       "      <td>17.0938</td>\n",
       "      <td>4.6106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_13</td>\n",
       "      <td>1</td>\n",
       "      <td>16.3699</td>\n",
       "      <td>1.5934</td>\n",
       "      <td>16.7395</td>\n",
       "      <td>7.3330</td>\n",
       "      <td>12.1450</td>\n",
       "      <td>5.9004</td>\n",
       "      <td>4.8222</td>\n",
       "      <td>20.9729</td>\n",
       "      <td>1.1064</td>\n",
       "      <td>8.6978</td>\n",
       "      <td>2.3287</td>\n",
       "      <td>-11.3409</td>\n",
       "      <td>13.7999</td>\n",
       "      <td>2.7925</td>\n",
       "      <td>6.3182</td>\n",
       "      <td>14.7313</td>\n",
       "      <td>7.2594</td>\n",
       "      <td>-2.4759</td>\n",
       "      <td>14.3984</td>\n",
       "      <td>9.1793</td>\n",
       "      <td>16.8467</td>\n",
       "      <td>19.4258</td>\n",
       "      <td>1.6565</td>\n",
       "      <td>2.5107</td>\n",
       "      <td>7.1272</td>\n",
       "      <td>13.6444</td>\n",
       "      <td>-12.8761</td>\n",
       "      <td>-1.0136</td>\n",
       "      <td>5.1773</td>\n",
       "      <td>7.3507</td>\n",
       "      <td>-16.0042</td>\n",
       "      <td>6.4460</td>\n",
       "      <td>-3.3238</td>\n",
       "      <td>14.3372</td>\n",
       "      <td>10.9481</td>\n",
       "      <td>6.7386</td>\n",
       "      <td>-0.5110</td>\n",
       "      <td>4.4335</td>\n",
       "      <td>...</td>\n",
       "      <td>32.5287</td>\n",
       "      <td>5.8451</td>\n",
       "      <td>4.9591</td>\n",
       "      <td>11.7805</td>\n",
       "      <td>-7.0936</td>\n",
       "      <td>11.8863</td>\n",
       "      <td>3.2597</td>\n",
       "      <td>-4.3478</td>\n",
       "      <td>7.5711</td>\n",
       "      <td>5.6656</td>\n",
       "      <td>3.5750</td>\n",
       "      <td>-8.6583</td>\n",
       "      <td>20.3131</td>\n",
       "      <td>-2.2723</td>\n",
       "      <td>15.7383</td>\n",
       "      <td>8.1730</td>\n",
       "      <td>-12.4963</td>\n",
       "      <td>15.1929</td>\n",
       "      <td>3.9751</td>\n",
       "      <td>8.5618</td>\n",
       "      <td>2.4566</td>\n",
       "      <td>9.3694</td>\n",
       "      <td>0.6942</td>\n",
       "      <td>11.9694</td>\n",
       "      <td>11.8982</td>\n",
       "      <td>0.6129</td>\n",
       "      <td>10.8123</td>\n",
       "      <td>-1.0803</td>\n",
       "      <td>11.9586</td>\n",
       "      <td>-0.5899</td>\n",
       "      <td>7.4002</td>\n",
       "      <td>7.4031</td>\n",
       "      <td>4.3989</td>\n",
       "      <td>4.0978</td>\n",
       "      <td>17.3638</td>\n",
       "      <td>-1.3022</td>\n",
       "      <td>9.6846</td>\n",
       "      <td>9.0419</td>\n",
       "      <td>15.6064</td>\n",
       "      <td>-10.8529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_14</td>\n",
       "      <td>0</td>\n",
       "      <td>13.8080</td>\n",
       "      <td>5.0514</td>\n",
       "      <td>17.2611</td>\n",
       "      <td>8.5120</td>\n",
       "      <td>12.8517</td>\n",
       "      <td>-9.1622</td>\n",
       "      <td>5.7327</td>\n",
       "      <td>21.0517</td>\n",
       "      <td>-4.5117</td>\n",
       "      <td>6.8116</td>\n",
       "      <td>8.2028</td>\n",
       "      <td>-7.8221</td>\n",
       "      <td>13.9241</td>\n",
       "      <td>10.3896</td>\n",
       "      <td>6.8838</td>\n",
       "      <td>14.1297</td>\n",
       "      <td>14.5268</td>\n",
       "      <td>-4.8877</td>\n",
       "      <td>26.1382</td>\n",
       "      <td>4.8558</td>\n",
       "      <td>23.4624</td>\n",
       "      <td>20.8922</td>\n",
       "      <td>8.0659</td>\n",
       "      <td>4.0123</td>\n",
       "      <td>7.6241</td>\n",
       "      <td>13.3559</td>\n",
       "      <td>-13.1966</td>\n",
       "      <td>-3.8157</td>\n",
       "      <td>6.1292</td>\n",
       "      <td>5.2018</td>\n",
       "      <td>-1.6535</td>\n",
       "      <td>11.3707</td>\n",
       "      <td>2.7550</td>\n",
       "      <td>19.5342</td>\n",
       "      <td>10.6458</td>\n",
       "      <td>10.0454</td>\n",
       "      <td>0.9564</td>\n",
       "      <td>3.8333</td>\n",
       "      <td>...</td>\n",
       "      <td>21.0429</td>\n",
       "      <td>5.7070</td>\n",
       "      <td>7.2320</td>\n",
       "      <td>13.8567</td>\n",
       "      <td>8.0575</td>\n",
       "      <td>9.5069</td>\n",
       "      <td>2.8172</td>\n",
       "      <td>-10.9940</td>\n",
       "      <td>4.0863</td>\n",
       "      <td>5.1196</td>\n",
       "      <td>-2.4897</td>\n",
       "      <td>3.1915</td>\n",
       "      <td>15.8254</td>\n",
       "      <td>3.8015</td>\n",
       "      <td>26.2032</td>\n",
       "      <td>8.5550</td>\n",
       "      <td>5.5689</td>\n",
       "      <td>10.5646</td>\n",
       "      <td>-1.1600</td>\n",
       "      <td>3.2684</td>\n",
       "      <td>-4.2496</td>\n",
       "      <td>10.5889</td>\n",
       "      <td>-15.4406</td>\n",
       "      <td>1.9688</td>\n",
       "      <td>30.8736</td>\n",
       "      <td>-6.4666</td>\n",
       "      <td>4.4271</td>\n",
       "      <td>16.0274</td>\n",
       "      <td>18.4116</td>\n",
       "      <td>1.0279</td>\n",
       "      <td>1.0740</td>\n",
       "      <td>8.3220</td>\n",
       "      <td>3.2619</td>\n",
       "      <td>1.6738</td>\n",
       "      <td>17.4797</td>\n",
       "      <td>-0.0257</td>\n",
       "      <td>-3.5323</td>\n",
       "      <td>9.3439</td>\n",
       "      <td>24.4479</td>\n",
       "      <td>-5.1110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_15</td>\n",
       "      <td>0</td>\n",
       "      <td>3.9416</td>\n",
       "      <td>2.6562</td>\n",
       "      <td>13.3633</td>\n",
       "      <td>6.8895</td>\n",
       "      <td>12.2806</td>\n",
       "      <td>-16.1620</td>\n",
       "      <td>5.6979</td>\n",
       "      <td>14.4573</td>\n",
       "      <td>-4.3144</td>\n",
       "      <td>7.1290</td>\n",
       "      <td>-7.0984</td>\n",
       "      <td>1.7324</td>\n",
       "      <td>14.1446</td>\n",
       "      <td>5.2403</td>\n",
       "      <td>5.0599</td>\n",
       "      <td>14.6456</td>\n",
       "      <td>7.2626</td>\n",
       "      <td>-15.3607</td>\n",
       "      <td>24.7424</td>\n",
       "      <td>17.6439</td>\n",
       "      <td>11.5724</td>\n",
       "      <td>11.4583</td>\n",
       "      <td>0.5490</td>\n",
       "      <td>2.3395</td>\n",
       "      <td>8.3642</td>\n",
       "      <td>14.2083</td>\n",
       "      <td>-7.7781</td>\n",
       "      <td>-2.0180</td>\n",
       "      <td>6.4194</td>\n",
       "      <td>3.6106</td>\n",
       "      <td>-12.7855</td>\n",
       "      <td>11.1948</td>\n",
       "      <td>1.3230</td>\n",
       "      <td>13.9853</td>\n",
       "      <td>10.7737</td>\n",
       "      <td>7.0264</td>\n",
       "      <td>1.6508</td>\n",
       "      <td>6.0989</td>\n",
       "      <td>...</td>\n",
       "      <td>17.7009</td>\n",
       "      <td>5.4925</td>\n",
       "      <td>4.6479</td>\n",
       "      <td>10.2630</td>\n",
       "      <td>1.9567</td>\n",
       "      <td>11.3951</td>\n",
       "      <td>2.0466</td>\n",
       "      <td>5.6207</td>\n",
       "      <td>6.3211</td>\n",
       "      <td>6.0198</td>\n",
       "      <td>7.9611</td>\n",
       "      <td>0.7532</td>\n",
       "      <td>8.4928</td>\n",
       "      <td>-9.9688</td>\n",
       "      <td>20.8953</td>\n",
       "      <td>8.3705</td>\n",
       "      <td>5.5334</td>\n",
       "      <td>8.5702</td>\n",
       "      <td>10.0225</td>\n",
       "      <td>1.4850</td>\n",
       "      <td>1.5508</td>\n",
       "      <td>9.1124</td>\n",
       "      <td>-4.4539</td>\n",
       "      <td>7.3382</td>\n",
       "      <td>4.2341</td>\n",
       "      <td>2.1422</td>\n",
       "      <td>9.4803</td>\n",
       "      <td>2.4272</td>\n",
       "      <td>17.7642</td>\n",
       "      <td>-0.1420</td>\n",
       "      <td>-3.4195</td>\n",
       "      <td>0.8829</td>\n",
       "      <td>-1.9859</td>\n",
       "      <td>3.9905</td>\n",
       "      <td>22.4647</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>6.5273</td>\n",
       "      <td>8.2899</td>\n",
       "      <td>12.9116</td>\n",
       "      <td>-4.9182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_16</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0615</td>\n",
       "      <td>0.2689</td>\n",
       "      <td>15.1325</td>\n",
       "      <td>3.6587</td>\n",
       "      <td>13.5276</td>\n",
       "      <td>-6.5477</td>\n",
       "      <td>5.2757</td>\n",
       "      <td>9.8710</td>\n",
       "      <td>2.5569</td>\n",
       "      <td>9.4701</td>\n",
       "      <td>-7.4401</td>\n",
       "      <td>-7.2719</td>\n",
       "      <td>14.1209</td>\n",
       "      <td>13.1612</td>\n",
       "      <td>7.2328</td>\n",
       "      <td>14.1264</td>\n",
       "      <td>12.9750</td>\n",
       "      <td>-13.1485</td>\n",
       "      <td>11.8312</td>\n",
       "      <td>8.5909</td>\n",
       "      <td>13.4809</td>\n",
       "      <td>25.1365</td>\n",
       "      <td>-1.8565</td>\n",
       "      <td>3.2490</td>\n",
       "      <td>10.1884</td>\n",
       "      <td>13.5710</td>\n",
       "      <td>-2.4240</td>\n",
       "      <td>-1.6708</td>\n",
       "      <td>6.2630</td>\n",
       "      <td>7.5248</td>\n",
       "      <td>-21.0856</td>\n",
       "      <td>5.9672</td>\n",
       "      <td>1.5934</td>\n",
       "      <td>12.7383</td>\n",
       "      <td>11.4781</td>\n",
       "      <td>5.6599</td>\n",
       "      <td>4.6793</td>\n",
       "      <td>6.5380</td>\n",
       "      <td>...</td>\n",
       "      <td>20.8226</td>\n",
       "      <td>5.7811</td>\n",
       "      <td>5.8153</td>\n",
       "      <td>8.6835</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>22.1404</td>\n",
       "      <td>2.9461</td>\n",
       "      <td>-8.2292</td>\n",
       "      <td>1.2020</td>\n",
       "      <td>5.9901</td>\n",
       "      <td>1.0695</td>\n",
       "      <td>-5.0337</td>\n",
       "      <td>13.3863</td>\n",
       "      <td>1.7808</td>\n",
       "      <td>30.1243</td>\n",
       "      <td>9.0178</td>\n",
       "      <td>-8.7409</td>\n",
       "      <td>9.3987</td>\n",
       "      <td>-10.5614</td>\n",
       "      <td>1.1569</td>\n",
       "      <td>-6.4217</td>\n",
       "      <td>12.7697</td>\n",
       "      <td>0.3566</td>\n",
       "      <td>8.2824</td>\n",
       "      <td>19.3315</td>\n",
       "      <td>1.5657</td>\n",
       "      <td>11.9624</td>\n",
       "      <td>15.2373</td>\n",
       "      <td>14.4534</td>\n",
       "      <td>-0.8986</td>\n",
       "      <td>0.2717</td>\n",
       "      <td>7.2854</td>\n",
       "      <td>0.5670</td>\n",
       "      <td>8.1822</td>\n",
       "      <td>19.5616</td>\n",
       "      <td>0.4365</td>\n",
       "      <td>-3.4975</td>\n",
       "      <td>8.6367</td>\n",
       "      <td>20.2548</td>\n",
       "      <td>11.1524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_17</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4199</td>\n",
       "      <td>-1.8128</td>\n",
       "      <td>8.1202</td>\n",
       "      <td>5.3955</td>\n",
       "      <td>9.7184</td>\n",
       "      <td>-17.8390</td>\n",
       "      <td>4.0959</td>\n",
       "      <td>15.2860</td>\n",
       "      <td>1.9016</td>\n",
       "      <td>7.0967</td>\n",
       "      <td>-9.0265</td>\n",
       "      <td>-13.1314</td>\n",
       "      <td>13.9721</td>\n",
       "      <td>9.7421</td>\n",
       "      <td>7.9172</td>\n",
       "      <td>14.5945</td>\n",
       "      <td>8.9762</td>\n",
       "      <td>7.0770</td>\n",
       "      <td>17.3135</td>\n",
       "      <td>-0.1572</td>\n",
       "      <td>11.1338</td>\n",
       "      <td>23.2228</td>\n",
       "      <td>0.9590</td>\n",
       "      <td>2.1129</td>\n",
       "      <td>12.6160</td>\n",
       "      <td>13.6211</td>\n",
       "      <td>-9.2049</td>\n",
       "      <td>-2.6749</td>\n",
       "      <td>6.0326</td>\n",
       "      <td>1.5041</td>\n",
       "      <td>-3.5015</td>\n",
       "      <td>10.6342</td>\n",
       "      <td>-6.7463</td>\n",
       "      <td>16.5111</td>\n",
       "      <td>11.5442</td>\n",
       "      <td>10.4791</td>\n",
       "      <td>1.1971</td>\n",
       "      <td>9.6804</td>\n",
       "      <td>...</td>\n",
       "      <td>40.5640</td>\n",
       "      <td>5.4060</td>\n",
       "      <td>2.6348</td>\n",
       "      <td>11.0620</td>\n",
       "      <td>-10.5800</td>\n",
       "      <td>10.0068</td>\n",
       "      <td>2.7462</td>\n",
       "      <td>0.9357</td>\n",
       "      <td>1.6138</td>\n",
       "      <td>5.5853</td>\n",
       "      <td>3.6524</td>\n",
       "      <td>-3.4627</td>\n",
       "      <td>13.6448</td>\n",
       "      <td>-15.3140</td>\n",
       "      <td>10.0939</td>\n",
       "      <td>6.1118</td>\n",
       "      <td>5.3184</td>\n",
       "      <td>16.2099</td>\n",
       "      <td>-10.3392</td>\n",
       "      <td>-2.0261</td>\n",
       "      <td>-0.4819</td>\n",
       "      <td>9.7169</td>\n",
       "      <td>-5.0330</td>\n",
       "      <td>8.2296</td>\n",
       "      <td>9.3832</td>\n",
       "      <td>4.0021</td>\n",
       "      <td>13.8037</td>\n",
       "      <td>6.8564</td>\n",
       "      <td>19.5097</td>\n",
       "      <td>0.9841</td>\n",
       "      <td>7.4363</td>\n",
       "      <td>11.3072</td>\n",
       "      <td>3.7903</td>\n",
       "      <td>0.0290</td>\n",
       "      <td>16.2954</td>\n",
       "      <td>0.1470</td>\n",
       "      <td>8.9603</td>\n",
       "      <td>9.4560</td>\n",
       "      <td>19.4505</td>\n",
       "      <td>-5.2407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_18</td>\n",
       "      <td>0</td>\n",
       "      <td>4.8750</td>\n",
       "      <td>1.2646</td>\n",
       "      <td>11.9190</td>\n",
       "      <td>8.4650</td>\n",
       "      <td>10.7203</td>\n",
       "      <td>-0.6707</td>\n",
       "      <td>5.6103</td>\n",
       "      <td>16.4661</td>\n",
       "      <td>-2.6601</td>\n",
       "      <td>8.4254</td>\n",
       "      <td>0.8679</td>\n",
       "      <td>-7.0238</td>\n",
       "      <td>13.9686</td>\n",
       "      <td>12.5741</td>\n",
       "      <td>6.2380</td>\n",
       "      <td>14.5437</td>\n",
       "      <td>12.2693</td>\n",
       "      <td>-13.1193</td>\n",
       "      <td>9.9427</td>\n",
       "      <td>17.9513</td>\n",
       "      <td>6.0450</td>\n",
       "      <td>10.4735</td>\n",
       "      <td>3.4739</td>\n",
       "      <td>2.1166</td>\n",
       "      <td>4.4838</td>\n",
       "      <td>14.0284</td>\n",
       "      <td>-8.3551</td>\n",
       "      <td>-0.7147</td>\n",
       "      <td>5.3071</td>\n",
       "      <td>5.4624</td>\n",
       "      <td>-18.1344</td>\n",
       "      <td>7.8493</td>\n",
       "      <td>-1.2431</td>\n",
       "      <td>15.6862</td>\n",
       "      <td>12.0081</td>\n",
       "      <td>12.3369</td>\n",
       "      <td>3.1091</td>\n",
       "      <td>11.0724</td>\n",
       "      <td>...</td>\n",
       "      <td>18.6760</td>\n",
       "      <td>5.4044</td>\n",
       "      <td>5.0521</td>\n",
       "      <td>10.8412</td>\n",
       "      <td>3.2416</td>\n",
       "      <td>21.5354</td>\n",
       "      <td>2.1884</td>\n",
       "      <td>-2.0804</td>\n",
       "      <td>8.0884</td>\n",
       "      <td>5.8174</td>\n",
       "      <td>1.8296</td>\n",
       "      <td>8.6585</td>\n",
       "      <td>16.0065</td>\n",
       "      <td>2.2611</td>\n",
       "      <td>10.8713</td>\n",
       "      <td>15.3639</td>\n",
       "      <td>4.0261</td>\n",
       "      <td>12.2528</td>\n",
       "      <td>8.0150</td>\n",
       "      <td>-0.7835</td>\n",
       "      <td>0.3129</td>\n",
       "      <td>12.5461</td>\n",
       "      <td>-4.0692</td>\n",
       "      <td>9.7048</td>\n",
       "      <td>12.6041</td>\n",
       "      <td>-8.5049</td>\n",
       "      <td>14.3691</td>\n",
       "      <td>-1.2482</td>\n",
       "      <td>18.9572</td>\n",
       "      <td>1.4226</td>\n",
       "      <td>9.1843</td>\n",
       "      <td>2.6812</td>\n",
       "      <td>1.8587</td>\n",
       "      <td>4.6929</td>\n",
       "      <td>16.0916</td>\n",
       "      <td>-1.5336</td>\n",
       "      <td>-4.0958</td>\n",
       "      <td>8.5873</td>\n",
       "      <td>17.1516</td>\n",
       "      <td>-22.1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_19</td>\n",
       "      <td>0</td>\n",
       "      <td>4.4090</td>\n",
       "      <td>-0.7863</td>\n",
       "      <td>15.1828</td>\n",
       "      <td>8.0631</td>\n",
       "      <td>11.2831</td>\n",
       "      <td>-0.7356</td>\n",
       "      <td>6.3801</td>\n",
       "      <td>16.0218</td>\n",
       "      <td>2.4621</td>\n",
       "      <td>8.2108</td>\n",
       "      <td>-1.7085</td>\n",
       "      <td>-15.0385</td>\n",
       "      <td>14.3029</td>\n",
       "      <td>6.4895</td>\n",
       "      <td>6.8158</td>\n",
       "      <td>15.1527</td>\n",
       "      <td>10.3103</td>\n",
       "      <td>-16.8120</td>\n",
       "      <td>20.3723</td>\n",
       "      <td>6.7767</td>\n",
       "      <td>9.4105</td>\n",
       "      <td>19.5561</td>\n",
       "      <td>5.8036</td>\n",
       "      <td>3.3431</td>\n",
       "      <td>14.6333</td>\n",
       "      <td>13.7211</td>\n",
       "      <td>1.5862</td>\n",
       "      <td>-2.0137</td>\n",
       "      <td>5.0974</td>\n",
       "      <td>3.1395</td>\n",
       "      <td>-13.8079</td>\n",
       "      <td>16.5406</td>\n",
       "      <td>-5.7001</td>\n",
       "      <td>15.1379</td>\n",
       "      <td>11.3475</td>\n",
       "      <td>-1.4485</td>\n",
       "      <td>2.8208</td>\n",
       "      <td>8.7190</td>\n",
       "      <td>...</td>\n",
       "      <td>42.2626</td>\n",
       "      <td>5.2130</td>\n",
       "      <td>2.6210</td>\n",
       "      <td>14.5142</td>\n",
       "      <td>-9.0844</td>\n",
       "      <td>13.4373</td>\n",
       "      <td>3.3911</td>\n",
       "      <td>6.3506</td>\n",
       "      <td>4.2669</td>\n",
       "      <td>5.6950</td>\n",
       "      <td>-7.8712</td>\n",
       "      <td>1.2484</td>\n",
       "      <td>11.3562</td>\n",
       "      <td>1.9240</td>\n",
       "      <td>25.2396</td>\n",
       "      <td>13.1753</td>\n",
       "      <td>0.7324</td>\n",
       "      <td>10.8040</td>\n",
       "      <td>-4.6552</td>\n",
       "      <td>3.3728</td>\n",
       "      <td>-5.2206</td>\n",
       "      <td>11.1295</td>\n",
       "      <td>-5.9693</td>\n",
       "      <td>6.1247</td>\n",
       "      <td>11.7440</td>\n",
       "      <td>-4.1421</td>\n",
       "      <td>7.3125</td>\n",
       "      <td>-7.4564</td>\n",
       "      <td>18.9102</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>5.9745</td>\n",
       "      <td>6.3267</td>\n",
       "      <td>2.5156</td>\n",
       "      <td>8.9878</td>\n",
       "      <td>16.0550</td>\n",
       "      <td>-2.1962</td>\n",
       "      <td>8.3985</td>\n",
       "      <td>9.1711</td>\n",
       "      <td>14.7352</td>\n",
       "      <td>3.9133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID_code  target    var_0   ...     var_197  var_198  var_199\n",
       "10  train_10       0   5.0702   ...      9.2553  14.2914  -7.6652\n",
       "11  train_11       0  12.7188   ...      9.6745  16.7498  -3.9728\n",
       "12  train_12       0   8.7671   ...      8.4897  17.0938   4.6106\n",
       "13  train_13       1  16.3699   ...      9.0419  15.6064 -10.8529\n",
       "14  train_14       0  13.8080   ...      9.3439  24.4479  -5.1110\n",
       "15  train_15       0   3.9416   ...      8.2899  12.9116  -4.9182\n",
       "16  train_16       0   5.0615   ...      8.6367  20.2548  11.1524\n",
       "17  train_17       0   8.4199   ...      9.4560  19.4505  -5.2407\n",
       "18  train_18       0   4.8750   ...      8.5873  17.1516 -22.1940\n",
       "19  train_19       0   4.4090   ...      9.1711  14.7352   3.9133\n",
       "\n",
       "[10 rows x 202 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "3160eb33db7ae8d18d5d938cde7c106cfe8b5362"
   },
   "outputs": [],
   "source": [
    "y = train['target']\n",
    "id_code_train = train['ID_code']\n",
    "id_code_test = test['ID_code']\n",
    "features = [c for c in train.columns if c not in ['ID_code','target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "289bc5e9ce799705615ab8b891e20094cfcbcab2"
   },
   "source": [
    "### 1.1 Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "204e12dc62034b9f86984896eb160ac1bace5c5d"
   },
   "outputs": [],
   "source": [
    "class GaussRankScaler():\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1e-9\n",
    "        self.lower = -1+self.epsilon\n",
    "        self.upper = 1 +self.epsilon\n",
    "        self.range = self.upper-self.lower\n",
    "        \n",
    "    def fit_transform(self,X):\n",
    "        i = np.argsort(X, axis=0)\n",
    "        j = np.argsort(i, axis=0)\n",
    "        \n",
    "        assert(j.min()==0).all()\n",
    "        assert(j.max()==len(j)-1).all()\n",
    "        \n",
    "        j_range = len( j ) - 1\n",
    "        self.divider = j_range / self.range\n",
    "\n",
    "        transformed = j / self.divider\n",
    "        transformed = transformed - self.upper\n",
    "        transformed = scipy.special.erfinv( transformed )\n",
    "        ############\n",
    "        # transformed = transformed - np.mean(transformed)\n",
    "\n",
    "        return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "f7d5bf3eb6347d833f4747ee554f392fdf00eaa4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n",
      "100%|██████████| 200/200 [00:26<00:00,  4.86it/s]\n",
      "100%|██████████| 400/400 [00:02<00:00, 153.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "(200000, 600)\n",
      "(200000, 600)\n"
     ]
    }
   ],
   "source": [
    "SPLIT = len(train)\n",
    "train = train.append(test)\n",
    "del test; gc.collect()  # trash \n",
    "\n",
    "scaler = GaussRankScaler()\n",
    "sc = StandardScaler()\n",
    "\n",
    "for feat in tqdm(features):\n",
    "    train[feat] = sc.fit_transform(train[feat].values.reshape(-1,1))\n",
    "    train[feat+'_r'] = rankdata(train[feat]).astype('float32')\n",
    "    train[feat+'_n'] = norm.cdf(train[feat]).astype('float32')\n",
    "    \n",
    "feats = [c for c in train.columns if c not in (['ID_code', 'target']+features)]\n",
    "\n",
    "for feat in tqdm(feats):\n",
    "    train[feat] = sc.fit_transform(train[feat].values.reshape(-1,1))\n",
    "    \n",
    "train = train.drop(['target','ID_code'], axis=1)\n",
    "test = train[SPLIT:].values\n",
    "train = train[:SPLIT].values\n",
    "\n",
    "print('Done')\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "#train[0:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "a8d5a51990fc76029535de4f35d6358689668d43"
   },
   "outputs": [],
   "source": [
    "train = np.reshape(train,(-1, NUM_FEATURES, 1))\n",
    "test = np.reshape(test,(-1, NUM_FEATURES, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "5664d206666ea12dc44833350a7f538905dd27b0"
   },
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train, y, stratify=y, test_size=0.2, random_state=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c18fd7f332cadfd630d91d3aa4a5552e2452f314"
   },
   "source": [
    "## 2-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "405768272ace3f3559cb80baef28d921e6aea2ad"
   },
   "outputs": [],
   "source": [
    "function = keras.layers.advanced_activations.LeakyReLU(alpha=.001)\n",
    "def create_model(input_shape, n_out):\n",
    "    input_tensor = Input(shape=input_shape)\n",
    "    x = Dense(16, activation=function)(input_tensor)\n",
    "    x = Flatten()(x)\n",
    "    out_put = Dense(n_out, activation='sigmoid')(x)\n",
    "    model = Model(input_tensor, out_put)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    return tf.py_func(roc_auc_score, (y_true, y_pred), tf.double)\n",
    "\n",
    "gamma = 2.0\n",
    "alpha=.25\n",
    "epsilon = K.epsilon()\n",
    "\n",
    "def focal_loss(y_true, y_pred):\n",
    "    pt_1 = y_pred * y_true\n",
    "    pt_1 = K.clip(pt_1, epsilon, 1-epsilon)\n",
    "    CE_1 = -K.log(pt_1)\n",
    "    FL_1 = alpha* K.pow(1-pt_1, gamma) * CE_1\n",
    "    \n",
    "    pt_0 = (1-y_pred) * (1-y_true)\n",
    "    pt_0 = K.clip(pt_0, epsilon, 1-epsilon)\n",
    "    CE_0 = -K.log(pt_0)\n",
    "    FL_0 = (1-alpha)* K.pow(1-pt_0, gamma) * CE_0\n",
    "    \n",
    "    loss = K.sum(FL_1, axis=1) + K.sum(FL_0, axis=1)\n",
    "    return loss\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0):\n",
    "    # y = np.array(y)\n",
    "    # print(y)\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    sample_size = x.shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    np.random.shuffle(index_array)\n",
    "    \n",
    "    mixed_x = lam * x + (1 - lam) * x[index_array]\n",
    "    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n",
    "    # print((1 - lam) * y[index_array])\n",
    "    # print((lam * y).shape,((1 - lam) * y[index_array]).shape)\n",
    "    return mixed_x, mixed_y\n",
    "\n",
    "def make_batches(size, batch_size):\n",
    "    nb_batch = int(np.ceil(size/float(batch_size)))\n",
    "    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n",
    "\n",
    "\n",
    "def batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n",
    "    y = np.array(y)\n",
    "    # print(X.shape[0], y.shape[0])\n",
    "    sample_size = X.shape[0]\n",
    "    index_array = np.arange(sample_size)\n",
    "    \n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(index_array)\n",
    "        batches = make_batches(sample_size, batch_size)\n",
    "        for batch_index, (batch_start, batch_end) in enumerate(batches):\n",
    "            batch_ids = index_array[batch_start:batch_end]\n",
    "            X_batch = X[batch_ids]\n",
    "            y_batch = y[batch_ids]\n",
    "            \n",
    "            if mixup:\n",
    "                # print('before', X_batch.shape, y_batch.shape)\n",
    "                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n",
    "            # print('*****************')    \n",
    "            yield X_batch,y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "59b31b3e5338bd3cfbdc2c0406c52da8c33fceaa"
   },
   "source": [
    "## 3- Feed forward NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "a3bd33428ec0ee4db2defa6185b5f00978cd72bc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/keras/activations.py:211: UserWarning: Do not pass a layer instance (such as LeakyReLU) as the activation argument of another layer. Instead, advanced activation layers should be used just like any other layer in a model.\n",
      "  identifier=identifier.__class__.__name__))\n",
      "/opt/conda/lib/python3.6/site-packages/keras/callbacks.py:1065: UserWarning: `epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
      "  warnings.warn('`epsilon` argument is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 600, 1)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 600, 16)           32        \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 9601      \n",
      "=================================================================\n",
      "Total params: 9,633\n",
      "Trainable params: 9,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "157/157 [==============================] - 23s 143ms/step - loss: 0.3061 - val_loss: 0.2529\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25294, saving model to feed_forward_model.h5\n",
      "Epoch 2/20\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.2777 - val_loss: 0.2457\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25294 to 0.24571, saving model to feed_forward_model.h5\n",
      "Epoch 3/20\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.2737 - val_loss: 0.2393\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.24571 to 0.23934, saving model to feed_forward_model.h5\n",
      "Epoch 4/20\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.2688 - val_loss: 0.2346\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.23934 to 0.23458, saving model to feed_forward_model.h5\n",
      "Epoch 5/20\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.2676 - val_loss: 0.2326\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.23458 to 0.23259, saving model to feed_forward_model.h5\n",
      "Epoch 6/20\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.2644 - val_loss: 0.2336\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.23259\n",
      "Epoch 7/20\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.2621 - val_loss: 0.2309\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.23259 to 0.23090, saving model to feed_forward_model.h5\n",
      "Epoch 8/20\n",
      "157/157 [==============================] - 22s 138ms/step - loss: 0.2611 - val_loss: 0.2260\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.23090 to 0.22604, saving model to feed_forward_model.h5\n",
      "Epoch 9/20\n",
      "157/157 [==============================] - 22s 141ms/step - loss: 0.2620 - val_loss: 0.2243\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.22604 to 0.22426, saving model to feed_forward_model.h5\n",
      "Epoch 10/20\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.2618 - val_loss: 0.2234\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.22426 to 0.22337, saving model to feed_forward_model.h5\n",
      "Epoch 11/20\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.2592 - val_loss: 0.2230\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.22337 to 0.22299, saving model to feed_forward_model.h5\n",
      "Epoch 12/20\n",
      "157/157 [==============================] - 23s 144ms/step - loss: 0.2594 - val_loss: 0.2423\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.22299\n",
      "Epoch 13/20\n",
      "157/157 [==============================] - 23s 144ms/step - loss: 0.2595 - val_loss: 0.2195\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.22299 to 0.21948, saving model to feed_forward_model.h5\n",
      "Epoch 14/20\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.2565 - val_loss: 0.2186\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.21948 to 0.21864, saving model to feed_forward_model.h5\n",
      "Epoch 15/20\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.2574 - val_loss: 0.2259\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.21864\n",
      "Epoch 16/20\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.2539 - val_loss: 0.2165\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.21864 to 0.21653, saving model to feed_forward_model.h5\n",
      "Epoch 17/20\n",
      "157/157 [==============================] - 21s 137ms/step - loss: 0.2542 - val_loss: 0.2174\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.21653\n",
      "Epoch 18/20\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.2569 - val_loss: 0.2144\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.21653 to 0.21437, saving model to feed_forward_model.h5\n",
      "Epoch 19/20\n",
      "157/157 [==============================] - 21s 136ms/step - loss: 0.2530 - val_loss: 0.2159\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.21437\n",
      "Epoch 20/20\n",
      "157/157 [==============================] - 22s 137ms/step - loss: 0.2535 - val_loss: 0.2147\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.21437\n"
     ]
    }
   ],
   "source": [
    "model = create_model((NUM_FEATURES,1),1)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "\n",
    "checkpoint = ModelCheckpoint('feed_forward_model.h5', monitor='val_loss', verbose=1, \n",
    "                             save_best_only=True, mode='min', save_weights_only = True)\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, \n",
    "                                   verbose=1, mode='min', epsilon=0.0001)\n",
    "early = EarlyStopping(monitor=\"val_loss\", \n",
    "                      mode=\"min\", \n",
    "                      patience=9)\n",
    "callbacks_list = [checkpoint, reduceLROnPlat, early]\n",
    "tr_gen = batch_generator(x_train,y_train,batch_size=BATCH_SIZE,shuffle=True,mixup=True)\n",
    "\n",
    "history = model.fit_generator(# x_train,y_train,\n",
    "                                tr_gen,\n",
    "                                steps_per_epoch=np.ceil(float(len(x_train)) / float(BATCH_SIZE)),\n",
    "                                epochs=20,\n",
    "                                verbose=1,\n",
    "                                callbacks=callbacks_list,\n",
    "                                validation_data=(x_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "98d0c3b954e55f1785f8e1960c00d6e8d8953a10",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s 30us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8842479618306468"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights('feed_forward_model.h5')\n",
    "prediction = model.predict(x_valid, batch_size=512, verbose=1)\n",
    "roc_auc_score(y_valid, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1598473e8bdf9a305b325b8ff2ebf6309cd33124"
   },
   "source": [
    "## 4- make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "861f8809ed4bf15346ea65e3f985a113fb3cba5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000/200000 [==============================] - 6s 30us/step\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "f1d073c125e571f30c37a277f5be91862503ec58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13750668],\n",
       "       [0.19902281],\n",
       "       [0.11948104],\n",
       "       ...,\n",
       "       [0.00236215],\n",
       "       [0.06136876],\n",
       "       [0.07921867]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "4e5dada5bbfd76025dac0b2b93c6b11ab036980b"
   },
   "outputs": [],
   "source": [
    "predict1 = np.round(predict,0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "0aeb097d3263cc0a219888cc2afc0f9023b63ae5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "bfad0827ec0a2e40e11ccf69cb5548b4f6986bfb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID_code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID_code  target\n",
       "0  test_0       0\n",
       "1  test_1       0\n",
       "2  test_2       0\n",
       "3  test_3       0\n",
       "4  test_4       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make submission\n",
    "submit=pd.read_csv('../input/sample_submission.csv')\n",
    "submit['ID_code'] = id_code_test\n",
    "submit['target'] = predict1\n",
    "submit.to_csv('submission.csv', index=False)\n",
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "6ccc806b0d8a752dbe5b55dd1da29bbf18c31a74"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "4d09a7e366f4416707f771dc8e0dfb8ffb02c96b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "40b274f1f7460ff4f91aaed84b0cbf3fe7523bae"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "30dbb6436e0d6e9c50ed408f7f14cd5508c9ec90"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "eb84e2e311d6f2d4a6a97482f4b517b4c89e9c1e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "20f9e183fc19a28b041d4b070abd4e219aec0484"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "fb8d28be6a2c57236f6e8c45c06fda8d14cda133"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "c597265bc920d4947022a100162082e96bc38167"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
