{
  "cells": [
    {
      "metadata": {
        "_uuid": "af5191e2569fc88ab5407b9150ff6d8a270dd560"
      },
      "cell_type": "markdown",
      "source": "# Prediction with NN\n> 2018-11-15 13:11:49  \n>\n> reference :  https://www.kaggle.com/jamesleslie/titanic-neural-network-for-beginners\n## About the missing values?\n* train's shape is (891, 12), 'Age' has 714 values, 'Cabin' has 204 values, 'Embarked' has 889 values.\n* final_test's shape is (418, 11), \"Age\" has 332 values, 'Fare' has 417 values, 'Cabin' has 91 values.\n\nFilling the missing age using average Title's age. Filling the missing Fare values using average Pclass's fare; Filling the missing embarked value as S.\n\nHavn't filled the missing Embarked. \n\n## Which factors can be used as input?\nCurrently, I used the 'Age','Fare','Parch','SibSp','Pclass','Sex_female','Sex_male',\n                    'Embarked_C','Embarked_Q','Embarked_S'\n\n## Tips\n\n What really helps in this is creating a good cross validation framework so you can get a reliable error estimate\n \n ## Value type\n* Numerical Features: Age, Fare, SibSp and Parch\n* Categorical Features: Sex, Embarked, Survived and Pclass\n* Alphanumeric Features: Ticket and Cabin(Contains both alphabets and the numeric value)\n* Text Features: Name"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf # NN\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#from sklearn.cross_validation import train_test_split\nfrom sklearn.model_selection import train_test_split # split train and test set\n\nfrom time import time\nimport os\nprint(os.listdir(\"../input\"))\n\n# read files\npath = '../input'\nfinal_test = pd.read_csv(path + '/test.csv') \ntrain = pd.read_csv(path + '/train.csv')\ndel path\n\npassengerId = final_test['PassengerId']  # hold it for the submission file\n\n# inspect data\nprint('train.shape',train.shape,\n      '\\nfinal_test.shape',final_test.shape)\n\nprint('====train info====')\nprint(train.info())\nprint('====final test info====')\nprint(final_test.info())\n#train.head(20)\n#final_test.head(20)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce48d06256becce8611b75057c024364aad38932"
      },
      "cell_type": "code",
      "source": "# data pre-processing. Since the limited info of cabin, I abandon this column.\n# missing age: use the average age of passenger's Title\n    \n# replace rare titles with more common ones\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr',\n           'Don': 'Mr', 'Mme': 'Miss', 'Jonkheer': 'Mr', 'Lady': 'Mrs',\n           'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ntitles = ['Dr', 'Master', 'Miss', 'Mr', 'Mrs', 'Rev']\n# function, extract title and creat new column for it, then filling missing age with average\ndef fill_missing_age(data):\n    #extract title, replace rare ones.\n    data[\"Title\"] = data[\"Name\"] # add a new column\n    for name in data[\"Title\"]:\n        data[\"Title\"] = data[\"Name\"].str.extract('([A-Za-z]+)\\.', expand=True)\n    data.replace({'Title': mapping}, inplace=True)\n    # present missing age with median of Title column\n    for title in titles:\n        median = data.groupby('Title')[\"Age\"].median()[titles.index(title)]\n        data.loc[(data['Age'].isnull()) & (data[\"Title\"] == title), 'Age'] = median\n    return data\n\ntrain = fill_missing_age(train)\nfinal_test = fill_missing_age(final_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "abe39a9a708094a8d04d3091fe2800d490ad5aea"
      },
      "cell_type": "code",
      "source": "# inspect the average age with Title\n#train.pivot_table(index='Title',values = 'Age').plot.bar()\n#plt.show()\n# the portion of survival with respect to Title\n#survived = train[train['Survived'] == 1]\n#died = train[train['Survived'] == 0]\n#sns.countplot(x='Title',data=train, palette='hls', hue='Survived')\n#plt.xticks(rotation=45)\n#plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3614c55bd0d76a6185fce1f4319649414b61578e"
      },
      "cell_type": "code",
      "source": "# For the missing Fare values, using the median fare value of class.\np_classes=[1,2,3]\ndef fill_missing_fare(data):\n    for p_class in p_classes:\n        median_fare = data.groupby('Pclass')['Fare'].median()[p_class]\n        data.loc[(data['Fare'].isnull()) & (data['Pclass'] == p_class),'Fare' ] = median_fare \n    return data\n\ntrain = fill_missing_fare(train)\nfinal_test = fill_missing_fare(final_test)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7634055e68a9ab09164f6abe16f0a8186c91df46"
      },
      "cell_type": "code",
      "source": "# filling missing embarked value with S\ntrain = train.fillna({\"Embarked\": \"S\"})",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e991884f2f96a513fb965ed654e5e54231990194"
      },
      "cell_type": "code",
      "source": "# Using one hot encoder to perform binarization of the category and include it as a feature\n# convert categorical variables into numeric format\n#categorical = data[['Pclass', 'Sex']].values\n#encoded_vars_temp = []\n\ndef create_dummies(data_df,column_name):\n    for name in column_name:        \n        dummies = pd.get_dummies(data_df[name], prefix=name)\n        data_df = pd.concat([data_df, dummies], axis=1)\n    return data_df\n\ntrain = create_dummies(train,['Pclass','Sex','Embarked'])\nfinal_test = create_dummies(final_test,['Pclass','Sex','Embarked']) \ntrain.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2f6717dd39dab73a80da5e17f60a3d3d2523e672",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "# Scale continuous variables to values between -1 and 1.\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler\n\nall_data = train.append(final_test, ignore_index=True)\n\ndef scale_var(df,column_names):\n    scaler = MinMaxScaler(feature_range=(-1.,1.))\n    for name in column_names:\n        var_scaled = scaler.fit_transform(df[[name]])\n        df[name] = var_scaled  \n    return df\nall_data = scale_var(all_data,['Age','Fare','SibSp','Parch'])\n# bring back to two datasets\n#all_data.loc[890:899]\n# refine data. reserve values for inputs and outputs.\nall_data = all_data [['Survived','Age','Fare','Parch','SibSp','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male',\n                    'Embarked_C','Embarked_Q','Embarked_S']]\n\ntrain = all_data[:890]\nfinal_test = all_data[891:]\nall_data",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "426b4226d8eacaf7a740567b95f730055a8c5823"
      },
      "cell_type": "code",
      "source": "# split the train to train and test, for cross validation\nfrom sklearn.model_selection import train_test_split\n\n# using train dataset for train and test the performance\ntrain_all_y = train[['Survived']]\ntrain_all_x = train[['Age','Fare','Parch','SibSp','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male',\n                    'Embarked_C','Embarked_Q','Embarked_S']]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d449f551550072a966b4468fc6b3aacddf758153"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "865f1c5d2ffa4fa916c8482105f14c8c7e892fb3"
      },
      "cell_type": "code",
      "source": "# test_size control the proportions of data are split into. Randomly split\ntr_train_X, tr_test_X, tr_train_y, tr_test_y = train_test_split(train_all_x, train_all_y, test_size = 0.4)\n# neural network\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\n# build neural network model\nmodel = Sequential()\nmodel.add(Dense(10, input_dim=tr_train_X.shape[1], activation='relu')) # input layer and hidden layer1 \nmodel.add(Dense(20, activation = 'relu'))\nmodel.add(Dense(15, activation = 'relu'))\nmodel.add(Dense(10, activation = 'relu'))\nmodel.add(Dense(1, activation='sigmoid')) # output layer\n# compile model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# train model\nmodel.fit(tr_train_X, tr_train_y,epochs=50, batch_size=32)\n# evaluate the model\nscores = model.evaluate(tr_train_X,tr_train_y)\nprint(\"\\n evaluate scores %s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n\nprediction = model.evaluate(tr_test_X, tr_test_y)\nprint(\"\\n prediction accuracy %s: %.3f%%\" % (model.metrics_names[1], prediction[1]*100))\n\n#result = model.predict(tr_test_X)\n#r = [round(x[0]) for x in result]\n#y = tr_test_y['Survived'].tolist()\n#diff = 0\n#for i in range(tr_test_y.shape[0]):\n#    diff = abs(r[i] - y[i]) + diff\n#print(\"\\n real acc:\", 1-(diff/tr_test_y.shape[0]) )\n\n#================== Gradient Boosting Classifier====================\n#tr_train_X, tr_test_X, tr_train_y, tr_test_y\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\ngbk = GradientBoostingClassifier()\ngbk.fit(tr_train_X, tr_train_y)\ny_pred = gbk.predict(tr_test_X)\nacc_gbk = round(accuracy_score(y_pred, tr_test_y) * 100, 2)\nprint('GradientBoostingClassifier accurate:',acc_gbk)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2691ff69c1edf1dd85d76da5e13ef2f85cb9bdb4"
      },
      "cell_type": "code",
      "source": "#Prediction with GBK\ngbk.fit(train_all_x, train_all_y)\nfinal_test_x = final_test[['Age','Fare','Parch','SibSp','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male',\n                    'Embarked_C','Embarked_Q','Embarked_S']]\npredictions = gbk.predict(final_test_x)\npredictions = predictions\nrounded = [round(x) for x in predictions]\nsolution = pd.DataFrame(rounded)\nsolution = solution.astype(int)\nsubmission = pd.concat([passengerId,solution], axis=1)\nsubmission.columns = ['PassengerId','Survived']\nsubmission.to_csv('Titanic_GBK_0.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ce8cb2b50c282d5d7847879c2498240f47a43d0d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cfcc8025f20a4856484d47abc898c203e68d2676"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1dbbe8193aa2c9400882ed925f832ea200c45b4"
      },
      "cell_type": "code",
      "source": "'''\n# datasets pre-processing finished\n# now split the train dataset to train and test, training neural network\n# apply all train data to fit model\n\n# train model\nmodel.fit(train_all_x,train_all_y,epochs=50, batch_size=32)\n\n# evaluate the model\nscores = model.evaluate(train_all_x,train_all_y)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5cdf5dbe433e85c3e7baae2e34d4c52fb7c224d0"
      },
      "cell_type": "code",
      "source": "'''\n# calculate predictions\nfinal_text_x = final_test[['Age','Fare','Parch','SibSp','Pclass_1','Pclass_2','Pclass_3','Sex_female','Sex_male',\n                    'Embarked_C','Embarked_Q','Embarked_S']]\npredictions = model.predict(final_text_x)\nrounded = [round(x[0]) for x in predictions]\n#rounded",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e49cb4c6cb43f5e25f7f663128351d47d22ec2bf"
      },
      "cell_type": "code",
      "source": "'''\nsolution = pd.DataFrame(rounded)\nsolution = solution.astype(int)\nsubmission = pd.concat([passengerId,solution], axis=1)\nsubmission.columns = ['PassengerId','Survived']\nsubmission.to_csv('Titanic_NN_4.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "243625130ae363f388aa1e33e931ed81bd78370a"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c44a1e4f274e16fa514d85542678c5a70cdcfdf"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fe4b673ca398b12a0aff35b99a91e31c6f4dde31"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}